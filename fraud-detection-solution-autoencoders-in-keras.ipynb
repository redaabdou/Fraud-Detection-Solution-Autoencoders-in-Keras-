{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will focus on how to detecte fraudulent transactions using Autoencoders in Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/creditcardfraud/creditcard.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, cohen_kappa_score\n",
    "from sklearn.metrics import f1_score, recall_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras import regularizers\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
       "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
       "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
       "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
       "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
       "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
       "\n",
       "        V26       V27       V28  Amount  Class  \n",
       "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('/kaggle/input/creditcardfraud/creditcard.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring in data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(284807, 31)\n",
      "Index(['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\n",
      "       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20',\n",
      "       'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount',\n",
      "       'Class'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "31 columns, 2 of which are Time and Amount. The rest are output from the PCA transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Letâ€™s check for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Not Fraud    284315\n",
       "Fraud           492\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.Class.value_counts().rename(index = {0:'Not Fraud', 1:'Fraud'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of 285k transactions just 492 were labelled as fraudulent, it is a small percentage but may represent billions of dollars of lost revenue each year."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PCA done on the dataset transformed it into standard-normal form. I will do the same to the 'time' and 'amount' columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Time'] = StandardScaler().fit_transform(data['Time'].values.reshape(-1, 1))\n",
    "data['Amount'] = StandardScaler().fit_transform(data['Amount'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we split the data into training and testing sets, And To evaluate the performance of our model we will training our model on the legitimate transactions,only, And Reserving the correct class on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, test_x = train_test_split(data,test_size = 0.3,random_state=42)\n",
    "train_x = train_x[train_x.Class == 0] \n",
    "train_x = train_x.drop(['Class'], axis=1) \n",
    "\n",
    "\n",
    "test_y = test_x['Class']\n",
    "test_x = test_x.drop(['Class'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Autoencoder uses 4 Desnse (fully connected) layers with 14, 7, 7 and 30 neurons respectively. The first two layers are used for our encoder, the last two go for the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = train_x.shape[1]\n",
    "encoding_dim = int(input_dim / 2) - 1\n",
    "hidden_dim = int(encoding_dim / 2)\n",
    "learning_rate = 1e-7\n",
    "\n",
    "input_layer = Input(shape=(input_dim, ))\n",
    "encoder = Dense(encoding_dim, activation=\"tanh\", activity_regularizer=regularizers.l1(learning_rate))(input_layer)\n",
    "encoder = Dense(hidden_dim, activation=\"relu\")(encoder)\n",
    "decoder = Dense(hidden_dim, activation='tanh')(encoder)\n",
    "decoder = Dense(input_dim, activation='relu')(decoder)\n",
    "autoencoder = Model(inputs=input_layer, outputs=decoder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train our model for 100 epochs with a batch size of 128 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_epoch = 100\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use Model Checkpoint to save the best model and TensorBoard for graph visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 199008 samples, validate on 85443 samples\n",
      "Epoch 1/100\n",
      "199008/199008 [==============================] - 4s 19us/step - loss: 0.8704 - acc: 0.4893 - val_loss: 0.8154 - val_acc: 0.5763\n",
      "Epoch 2/100\n",
      "199008/199008 [==============================] - 3s 15us/step - loss: 0.7772 - acc: 0.5945 - val_loss: 0.7827 - val_acc: 0.6072\n",
      "Epoch 3/100\n",
      "199008/199008 [==============================] - 3s 15us/step - loss: 0.7547 - acc: 0.6105 - val_loss: 0.7688 - val_acc: 0.6159\n",
      "Epoch 4/100\n",
      "199008/199008 [==============================] - 3s 15us/step - loss: 0.7424 - acc: 0.6177 - val_loss: 0.7588 - val_acc: 0.6227\n",
      "Epoch 5/100\n",
      "199008/199008 [==============================] - 3s 15us/step - loss: 0.7347 - acc: 0.6274 - val_loss: 0.7531 - val_acc: 0.6347\n",
      "Epoch 6/100\n",
      "199008/199008 [==============================] - 3s 15us/step - loss: 0.7290 - acc: 0.6377 - val_loss: 0.7481 - val_acc: 0.6456\n",
      "Epoch 7/100\n",
      "199008/199008 [==============================] - 3s 16us/step - loss: 0.7242 - acc: 0.6477 - val_loss: 0.7451 - val_acc: 0.6537\n",
      "Epoch 8/100\n",
      "199008/199008 [==============================] - 3s 15us/step - loss: 0.7209 - acc: 0.6542 - val_loss: 0.7420 - val_acc: 0.6578\n",
      "Epoch 9/100\n",
      "199008/199008 [==============================] - 3s 15us/step - loss: 0.7184 - acc: 0.6585 - val_loss: 0.7403 - val_acc: 0.6623\n",
      "Epoch 10/100\n",
      "199008/199008 [==============================] - 3s 15us/step - loss: 0.7163 - acc: 0.6612 - val_loss: 0.7390 - val_acc: 0.6655\n",
      "Epoch 11/100\n",
      "199008/199008 [==============================] - 3s 16us/step - loss: 0.7146 - acc: 0.6647 - val_loss: 0.7370 - val_acc: 0.6647\n",
      "Epoch 12/100\n",
      "199008/199008 [==============================] - 3s 15us/step - loss: 0.7133 - acc: 0.6657 - val_loss: 0.7359 - val_acc: 0.6663\n",
      "Epoch 13/100\n",
      "199008/199008 [==============================] - 3s 15us/step - loss: 0.7119 - acc: 0.6679 - val_loss: 0.7353 - val_acc: 0.6673\n",
      "Epoch 14/100\n",
      "199008/199008 [==============================] - 3s 16us/step - loss: 0.7110 - acc: 0.6695 - val_loss: 0.7340 - val_acc: 0.6723\n",
      "Epoch 15/100\n",
      "199008/199008 [==============================] - 3s 16us/step - loss: 0.7098 - acc: 0.6708 - val_loss: 0.7334 - val_acc: 0.6785\n",
      "Epoch 16/100\n",
      "199008/199008 [==============================] - 3s 15us/step - loss: 0.7088 - acc: 0.6714 - val_loss: 0.7330 - val_acc: 0.6719\n",
      "Epoch 17/100\n",
      "199008/199008 [==============================] - 3s 15us/step - loss: 0.7082 - acc: 0.6730 - val_loss: 0.7313 - val_acc: 0.6743\n",
      "Epoch 18/100\n",
      "199008/199008 [==============================] - 3s 17us/step - loss: 0.7073 - acc: 0.6742 - val_loss: 0.7309 - val_acc: 0.6721\n",
      "Epoch 19/100\n",
      "199008/199008 [==============================] - 3s 16us/step - loss: 0.7069 - acc: 0.6755 - val_loss: 0.7308 - val_acc: 0.6723\n",
      "Epoch 20/100\n",
      "199008/199008 [==============================] - 3s 15us/step - loss: 0.7062 - acc: 0.6757 - val_loss: 0.7308 - val_acc: 0.6782\n",
      "Epoch 21/100\n",
      "199008/199008 [==============================] - 3s 15us/step - loss: 0.7060 - acc: 0.6773 - val_loss: 0.7320 - val_acc: 0.6755\n",
      "Epoch 22/100\n",
      "199008/199008 [==============================] - 3s 15us/step - loss: 0.7053 - acc: 0.6777 - val_loss: 0.7290 - val_acc: 0.6776\n",
      "Epoch 23/100\n",
      "199008/199008 [==============================] - 3s 16us/step - loss: 0.7048 - acc: 0.6781 - val_loss: 0.7288 - val_acc: 0.6839\n",
      "Epoch 24/100\n",
      "199008/199008 [==============================] - 3s 17us/step - loss: 0.7047 - acc: 0.6796 - val_loss: 0.7277 - val_acc: 0.6791\n",
      "Epoch 25/100\n",
      "199008/199008 [==============================] - 3s 16us/step - loss: 0.7042 - acc: 0.6795 - val_loss: 0.7277 - val_acc: 0.6821\n",
      "Epoch 26/100\n",
      "199008/199008 [==============================] - 3s 16us/step - loss: 0.7042 - acc: 0.6796 - val_loss: 0.7279 - val_acc: 0.6816\n",
      "Epoch 27/100\n",
      "199008/199008 [==============================] - 3s 15us/step - loss: 0.7035 - acc: 0.6791 - val_loss: 0.7269 - val_acc: 0.6823\n",
      "Epoch 28/100\n",
      "199008/199008 [==============================] - 3s 16us/step - loss: 0.7034 - acc: 0.6801 - val_loss: 0.7275 - val_acc: 0.6807\n",
      "Epoch 29/100\n",
      "199008/199008 [==============================] - 3s 15us/step - loss: 0.7031 - acc: 0.6797 - val_loss: 0.7281 - val_acc: 0.6789\n",
      "Epoch 30/100\n",
      "199008/199008 [==============================] - 3s 16us/step - loss: 0.7029 - acc: 0.6807 - val_loss: 0.7274 - val_acc: 0.6769\n",
      "Epoch 31/100\n",
      "199008/199008 [==============================] - 3s 16us/step - loss: 0.7032 - acc: 0.6807 - val_loss: 0.7266 - val_acc: 0.6812\n",
      "Epoch 32/100\n",
      "199008/199008 [==============================] - 3s 16us/step - loss: 0.7022 - acc: 0.6817 - val_loss: 0.7264 - val_acc: 0.6790\n",
      "Epoch 33/100\n",
      "199008/199008 [==============================] - 3s 16us/step - loss: 0.7020 - acc: 0.6821 - val_loss: 0.7255 - val_acc: 0.6806\n",
      "Epoch 34/100\n",
      "199008/199008 [==============================] - 3s 16us/step - loss: 0.7013 - acc: 0.6834 - val_loss: 0.7284 - val_acc: 0.6804\n",
      "Epoch 35/100\n",
      "199008/199008 [==============================] - 3s 16us/step - loss: 0.7010 - acc: 0.6827 - val_loss: 0.7243 - val_acc: 0.6846\n",
      "Epoch 36/100\n",
      "199008/199008 [==============================] - 3s 16us/step - loss: 0.7011 - acc: 0.6817 - val_loss: 0.7242 - val_acc: 0.6828\n",
      "Epoch 37/100\n",
      "199008/199008 [==============================] - 3s 15us/step - loss: 0.7005 - acc: 0.6816 - val_loss: 0.7254 - val_acc: 0.6808\n",
      "Epoch 38/100\n",
      "199008/199008 [==============================] - 3s 16us/step - loss: 0.7003 - acc: 0.6820 - val_loss: 0.7230 - val_acc: 0.6838\n",
      "Epoch 39/100\n",
      "199008/199008 [==============================] - 3s 16us/step - loss: 0.6997 - acc: 0.6824 - val_loss: 0.7262 - val_acc: 0.6840\n",
      "Epoch 40/100\n",
      "199008/199008 [==============================] - 3s 15us/step - loss: 0.6996 - acc: 0.6823 - val_loss: 0.7297 - val_acc: 0.6778\n",
      "Epoch 41/100\n",
      "199008/199008 [==============================] - 3s 15us/step - loss: 0.6998 - acc: 0.6813 - val_loss: 0.7232 - val_acc: 0.6840\n",
      "Epoch 42/100\n",
      "199008/199008 [==============================] - 3s 15us/step - loss: 0.6993 - acc: 0.6818 - val_loss: 0.7242 - val_acc: 0.6812\n",
      "Epoch 43/100\n",
      "199008/199008 [==============================] - 3s 16us/step - loss: 0.6991 - acc: 0.6821 - val_loss: 0.7227 - val_acc: 0.6861\n",
      "Epoch 44/100\n",
      "199008/199008 [==============================] - 3s 16us/step - loss: 0.6986 - acc: 0.6822 - val_loss: 0.7216 - val_acc: 0.6822\n",
      "Epoch 45/100\n",
      "199008/199008 [==============================] - 3s 15us/step - loss: 0.6986 - acc: 0.6828 - val_loss: 0.7267 - val_acc: 0.6829\n",
      "Epoch 46/100\n",
      "199008/199008 [==============================] - 3s 15us/step - loss: 0.6987 - acc: 0.6830 - val_loss: 0.7225 - val_acc: 0.6796\n",
      "Epoch 47/100\n",
      "199008/199008 [==============================] - 3s 15us/step - loss: 0.6987 - acc: 0.6832 - val_loss: 0.7213 - val_acc: 0.6800\n",
      "Epoch 48/100\n",
      "199008/199008 [==============================] - 3s 16us/step - loss: 0.6983 - acc: 0.6838 - val_loss: 0.7213 - val_acc: 0.6863\n",
      "Epoch 49/100\n",
      "199008/199008 [==============================] - 3s 15us/step - loss: 0.6982 - acc: 0.6840 - val_loss: 0.7214 - val_acc: 0.6795\n",
      "Epoch 50/100\n",
      "199008/199008 [==============================] - 3s 15us/step - loss: 0.6981 - acc: 0.6837 - val_loss: 0.7228 - val_acc: 0.6791\n",
      "Epoch 51/100\n",
      "199008/199008 [==============================] - 3s 16us/step - loss: 0.6980 - acc: 0.6844 - val_loss: 0.7200 - val_acc: 0.6876\n",
      "Epoch 52/100\n",
      "199008/199008 [==============================] - 3s 15us/step - loss: 0.6981 - acc: 0.6852 - val_loss: 0.7219 - val_acc: 0.6800\n",
      "Epoch 53/100\n",
      "199008/199008 [==============================] - 3s 15us/step - loss: 0.6977 - acc: 0.6847 - val_loss: 0.7209 - val_acc: 0.6869\n",
      "Epoch 54/100\n",
      "199008/199008 [==============================] - 3s 15us/step - loss: 0.6977 - acc: 0.6855 - val_loss: 0.7212 - val_acc: 0.6803\n",
      "Epoch 55/100\n",
      "199008/199008 [==============================] - 3s 16us/step - loss: 0.6977 - acc: 0.6854 - val_loss: 0.7217 - val_acc: 0.6823\n",
      "Epoch 56/100\n",
      "199008/199008 [==============================] - 3s 15us/step - loss: 0.6976 - acc: 0.6858 - val_loss: 0.7218 - val_acc: 0.6840\n",
      "Epoch 57/100\n",
      "199008/199008 [==============================] - 3s 15us/step - loss: 0.6975 - acc: 0.6856 - val_loss: 0.7206 - val_acc: 0.6862\n",
      "Epoch 58/100\n",
      "199008/199008 [==============================] - 3s 16us/step - loss: 0.6976 - acc: 0.6860 - val_loss: 0.7202 - val_acc: 0.6893\n",
      "Epoch 59/100\n",
      "199008/199008 [==============================] - 3s 15us/step - loss: 0.6975 - acc: 0.6865 - val_loss: 0.7218 - val_acc: 0.6824\n",
      "Epoch 60/100\n",
      "199008/199008 [==============================] - 3s 15us/step - loss: 0.6972 - acc: 0.6866 - val_loss: 0.7210 - val_acc: 0.6839\n",
      "Epoch 61/100\n",
      "199008/199008 [==============================] - 3s 15us/step - loss: 0.6970 - acc: 0.6873 - val_loss: 0.7206 - val_acc: 0.6836\n",
      "Epoch 62/100\n",
      "199008/199008 [==============================] - 3s 15us/step - loss: 0.6974 - acc: 0.6871 - val_loss: 0.7206 - val_acc: 0.6865\n",
      "Epoch 63/100\n",
      "199008/199008 [==============================] - 3s 15us/step - loss: 0.6972 - acc: 0.6870 - val_loss: 0.7200 - val_acc: 0.6866\n",
      "Epoch 64/100\n",
      "199008/199008 [==============================] - 3s 15us/step - loss: 0.6966 - acc: 0.6877 - val_loss: 0.7238 - val_acc: 0.6907\n",
      "Epoch 65/100\n",
      "199008/199008 [==============================] - 3s 16us/step - loss: 0.6969 - acc: 0.6882 - val_loss: 0.7202 - val_acc: 0.6912\n",
      "Epoch 66/100\n",
      "199008/199008 [==============================] - 3s 15us/step - loss: 0.6969 - acc: 0.6881 - val_loss: 0.7212 - val_acc: 0.6887\n",
      "Epoch 67/100\n",
      "199008/199008 [==============================] - 3s 15us/step - loss: 0.6967 - acc: 0.6884 - val_loss: 0.7276 - val_acc: 0.6806\n",
      "Epoch 68/100\n",
      "199008/199008 [==============================] - 3s 15us/step - loss: 0.6970 - acc: 0.6880 - val_loss: 0.7208 - val_acc: 0.6896\n",
      "Epoch 69/100\n",
      "199008/199008 [==============================] - 3s 15us/step - loss: 0.6971 - acc: 0.6885 - val_loss: 0.7208 - val_acc: 0.6869\n",
      "Epoch 70/100\n",
      "199008/199008 [==============================] - 3s 16us/step - loss: 0.6967 - acc: 0.6885 - val_loss: 0.7219 - val_acc: 0.6967\n",
      "Epoch 71/100\n",
      "199008/199008 [==============================] - 3s 16us/step - loss: 0.6968 - acc: 0.6890 - val_loss: 0.7202 - val_acc: 0.6877\n",
      "Epoch 72/100\n",
      "199008/199008 [==============================] - 3s 15us/step - loss: 0.6964 - acc: 0.6908 - val_loss: 0.7208 - val_acc: 0.6939\n",
      "Epoch 73/100\n",
      "199008/199008 [==============================] - 3s 16us/step - loss: 0.6964 - acc: 0.6900 - val_loss: 0.7196 - val_acc: 0.6902\n",
      "Epoch 74/100\n",
      "199008/199008 [==============================] - 3s 16us/step - loss: 0.6965 - acc: 0.6901 - val_loss: 0.7199 - val_acc: 0.6899\n",
      "Epoch 75/100\n",
      "199008/199008 [==============================] - 3s 17us/step - loss: 0.6963 - acc: 0.6907 - val_loss: 0.7201 - val_acc: 0.6906\n",
      "Epoch 76/100\n",
      "199008/199008 [==============================] - 3s 17us/step - loss: 0.6960 - acc: 0.6910 - val_loss: 0.7199 - val_acc: 0.6952\n",
      "Epoch 77/100\n",
      "199008/199008 [==============================] - 3s 15us/step - loss: 0.6965 - acc: 0.6903 - val_loss: 0.7240 - val_acc: 0.6852\n",
      "Epoch 78/100\n",
      "199008/199008 [==============================] - 3s 15us/step - loss: 0.6966 - acc: 0.6907 - val_loss: 0.7198 - val_acc: 0.6905\n",
      "Epoch 79/100\n",
      "199008/199008 [==============================] - 3s 15us/step - loss: 0.6964 - acc: 0.6917 - val_loss: 0.7190 - val_acc: 0.6919\n",
      "Epoch 80/100\n",
      "199008/199008 [==============================] - 3s 15us/step - loss: 0.6961 - acc: 0.6914 - val_loss: 0.7208 - val_acc: 0.6881\n",
      "Epoch 81/100\n",
      "199008/199008 [==============================] - 3s 16us/step - loss: 0.6962 - acc: 0.6910 - val_loss: 0.7245 - val_acc: 0.6886\n",
      "Epoch 82/100\n",
      "199008/199008 [==============================] - 3s 16us/step - loss: 0.6963 - acc: 0.6917 - val_loss: 0.7221 - val_acc: 0.6945\n",
      "Epoch 83/100\n",
      "199008/199008 [==============================] - 3s 16us/step - loss: 0.6956 - acc: 0.6921 - val_loss: 0.7193 - val_acc: 0.6926\n",
      "Epoch 84/100\n",
      "199008/199008 [==============================] - 3s 17us/step - loss: 0.6955 - acc: 0.6918 - val_loss: 0.7193 - val_acc: 0.6881\n",
      "Epoch 85/100\n",
      "199008/199008 [==============================] - 3s 16us/step - loss: 0.6956 - acc: 0.6920 - val_loss: 0.7318 - val_acc: 0.6976\n",
      "Epoch 86/100\n",
      "199008/199008 [==============================] - 3s 16us/step - loss: 0.6957 - acc: 0.6923 - val_loss: 0.7197 - val_acc: 0.6928\n",
      "Epoch 87/100\n",
      "199008/199008 [==============================] - 3s 15us/step - loss: 0.6957 - acc: 0.6915 - val_loss: 0.7192 - val_acc: 0.6913\n",
      "Epoch 88/100\n",
      "199008/199008 [==============================] - 3s 16us/step - loss: 0.6957 - acc: 0.6918 - val_loss: 0.7187 - val_acc: 0.6956\n",
      "Epoch 89/100\n",
      "199008/199008 [==============================] - 3s 15us/step - loss: 0.6953 - acc: 0.6919 - val_loss: 0.7192 - val_acc: 0.6921\n",
      "Epoch 90/100\n",
      "199008/199008 [==============================] - 3s 16us/step - loss: 0.6955 - acc: 0.6930 - val_loss: 0.7199 - val_acc: 0.6919\n",
      "Epoch 91/100\n",
      "199008/199008 [==============================] - 3s 16us/step - loss: 0.6955 - acc: 0.6928 - val_loss: 0.7183 - val_acc: 0.6910\n",
      "Epoch 92/100\n",
      "199008/199008 [==============================] - 3s 15us/step - loss: 0.6951 - acc: 0.6929 - val_loss: 0.7184 - val_acc: 0.6913\n",
      "Epoch 93/100\n",
      "199008/199008 [==============================] - 3s 17us/step - loss: 0.6953 - acc: 0.6928 - val_loss: 0.7214 - val_acc: 0.6967\n",
      "Epoch 94/100\n",
      "199008/199008 [==============================] - 4s 18us/step - loss: 0.6954 - acc: 0.6919 - val_loss: 0.7195 - val_acc: 0.6894\n",
      "Epoch 95/100\n",
      "199008/199008 [==============================] - 4s 19us/step - loss: 0.6951 - acc: 0.6922 - val_loss: 0.7186 - val_acc: 0.6898\n",
      "Epoch 96/100\n",
      "199008/199008 [==============================] - 4s 21us/step - loss: 0.6952 - acc: 0.6920 - val_loss: 0.7190 - val_acc: 0.6879\n",
      "Epoch 97/100\n",
      "199008/199008 [==============================] - 4s 20us/step - loss: 0.6948 - acc: 0.6922 - val_loss: 0.7190 - val_acc: 0.6941\n",
      "Epoch 98/100\n",
      "199008/199008 [==============================] - 4s 19us/step - loss: 0.6948 - acc: 0.6932 - val_loss: 0.7213 - val_acc: 0.6974\n",
      "Epoch 99/100\n",
      "199008/199008 [==============================] - 4s 19us/step - loss: 0.6953 - acc: 0.6920 - val_loss: 0.7182 - val_acc: 0.6999\n",
      "Epoch 100/100\n",
      "199008/199008 [==============================] - 4s 19us/step - loss: 0.6947 - acc: 0.6929 - val_loss: 0.7184 - val_acc: 0.6960\n"
     ]
    }
   ],
   "source": [
    "autoencoder.compile(metrics=['accuracy'],\n",
    "                    loss='mean_squared_error',\n",
    "                    optimizer='adam')\n",
    "\n",
    "cp = ModelCheckpoint(filepath=\"autoencoder_fraud.h5\",\n",
    "                               save_best_only=True,\n",
    "                               verbose=0)\n",
    "\n",
    "tb = TensorBoard(log_dir='./logs',\n",
    "                histogram_freq=0,\n",
    "                write_graph=True,\n",
    "                write_images=True)\n",
    "\n",
    "history = autoencoder.fit(train_x, train_x,\n",
    "                    epochs=nb_epoch,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=True,\n",
    "                    validation_data=(test_x, test_x),\n",
    "                    verbose=1,\n",
    "                    callbacks=[cp, tb]).history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = load_model('autoencoder_fraud.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Visualization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history['loss'], linewidth=2, label='Train')\n",
    "plt.plot(history['val_loss'], linewidth=2, label='Test')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "seems like our model work nicely, now we will make the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = autoencoder.predict(test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = np.mean(np.power(test_x - pred, 2), axis=1)\n",
    "error_df = pd.DataFrame({'Reconstruction_error': mse,\n",
    "                        'True_class': test_y})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([77.24055566,  0.87190388,  0.24955684, ...,  0.51939849,\n",
       "        0.24887566,  0.1524689 ])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_df.Reconstruction_error.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we will use a threshold to separate between fraudulent transactions and legitimate transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_fixed = 5\n",
    "pred_y = [1 if e > threshold_fixed else 0 for e in error_df.Reconstruction_error.values]\n",
    "matrix = confusion_matrix(error_df.True_class, pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpos = matrix[0][0]\n",
    "fneg = matrix[1][1]\n",
    "fpos = matrix[0][1]\n",
    "tneg = matrix[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 98.88%\n",
      "Cohen Kappa: 0.162\n",
      "Sensitivity/Recall for Model : 0.69\n",
      "F1 Score for Model : 0.16\n"
     ]
    }
   ],
   "source": [
    "print( 'Accuracy: '+ str(np.round(100*float(tpos+fneg)/float(tpos+fneg + fpos + tneg),2))+'%')\n",
    "print( 'Cohen Kappa: '+ str(np.round(cohen_kappa_score(error_df.True_class, pred_y),3)))\n",
    "print(\"Sensitivity/Recall for Model : {}\".format(round(recall_score(error_df.True_class, pred_y), 2)))\n",
    "print(\"F1 Score for Model : {}\".format(round(f1_score(error_df.True_class, pred_y), 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This work is inspired by : https://www.datascience.com/blog/fraud-detection-with-tensorflow"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
